# Acoustic Reef ğŸŒŠğŸ¶  
**AI-powered stethoscope for the ocean**

---

## ğŸ“Œ Mission
Acoustic Reef is an AI-powered platform that analyzes underwater soundscapes to assess coral reef health in real time. Healthy reefs are noisy with life, while degraded reefs fall silent â€” we use this sound signature as a vital sign.

---

## ğŸš© Problem
Coral reefs are collapsing globally, and traditional monitoring methods (manual dive surveys) are too slow, expensive, and unscalable to provide the necessary early warnings for conservation efforts.

---

## ğŸ’¡ Our Solution
- **Upload Audio**: Users upload .wav files from underwater microphones (hydrophones)
- **AI Analysis**: Multi-output model using Google SurfPerch + custom scikit-learn classifier
- **Reef Vital Signs Report**: Simultaneous classification of:
  - Reef health status (Healthy/Degraded)
  - Anthrophony detection (human-made noise presence)
- **Dashboard**: Simple web interface built with Streamlit

---

## ğŸ”§ Tech Stack
- **Language**: Python
- **Web Dashboard**: Streamlit
- **AI Model Training/Usage**: Kaggle Notebooks, TensorFlow, scikit-learn
- **Audio & Data**: Librosa, Pandas, NumPy
- **Foundation Model**: Google SurfPerch (pre-trained, hosted on Kaggle)

---

## ğŸ¯ Unique Value Proposition
Our innovation is democratizing powerful but inaccessible marine science. We are building the first user-friendly platform designed for non-experts like park rangers, conservation NGOs, and local communities.

---

## ğŸ“‚ Project Structure
```
acoustic-reef/
â”œâ”€â”€ data/                    # Audio datasets and samples
â”‚   â”œâ”€â”€ raw/                # Original audio files
â”‚   â”œâ”€â”€ processed/          # Preprocessed audio data
â”‚   â””â”€â”€ embeddings/         # SurfPerch embeddings
â”œâ”€â”€ models/                 # Trained models and weights
â”‚   â”œâ”€â”€ surfperch/          # SurfPerch model files
â”‚   â””â”€â”€ classifiers/        # Custom scikit-learn models
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ audio/             # Audio processing utilities
â”‚   â”œâ”€â”€ models/            # Model training and inference
â”‚   â””â”€â”€ utils/              # General utilities
â”œâ”€â”€ notebooks/              # Kaggle notebooks for model development
â”‚   â”œâ”€â”€ surfperch/         # SurfPerch integration
â”‚   â”œâ”€â”€ training/          # Model training notebooks
â”‚   â””â”€â”€ evaluation/        # Model evaluation notebooks
â”œâ”€â”€ dashboard/              # Streamlit web application
â”‚   â”œâ”€â”€ app.py             # Main Streamlit app
â”‚   â”œâ”€â”€ components/        # UI components
â”‚   â””â”€â”€ pages/             # Multi-page structure
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md              # Project documentation
```

---

## ğŸ‘©â€ğŸ’» Team
- **Project Lead & Integrator**: Managing GitHub, sprint planning, and final AI model integration
- **Data Lead**: TBD  
- **AI/ML Lead**: TBD  
- **UI/UX Lead**: TBD  

---

## ğŸš€ Getting Started
```bash
git clone https://github.com/your-username/acoustic-reef.git
cd acoustic-reef

pip install -r requirements.txt

streamlit run dashboard/app.py
```

---

## ğŸ”¬ Technical Approach
1. **Foundation Model**: Use pre-trained Google SurfPerch model from Kaggle
2. **Embeddings**: Generate audio embeddings using TensorFlow saved_model format
3. **Custom Classifier**: Train scikit-learn classifier on SurfPerch embeddings
4. **Multi-Output**: Simultaneous reef health and anthrophony classification
5. **Web Interface**: Streamlit dashboard for user interaction
